[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "üëã Hi, I‚Äôm JT, from DSD. We help people understand and use data models.\nüëÄ Key areas of research and development are statistics, AI-ML, and cyber resiliency.\n I write regular features on Medium and Substack.\nüå± Let‚Äôs help you find more and better ways to analyze data with R, Python and SQL.\nüíûÔ∏è Your clients can benefit from our collaborations on machine learning applied for strategic insights, statistical analysis and data cleaning.\nüì´ Let‚Äôs communicate: millerauthor@datascientistdude.com for business inquiries please.\nüòÑ Dad Joke of the Day: One day I gave up my seat on public transportation for a blind lady. And that is how I lost my job as a bus driver."
  },
  {
    "objectID": "posts/workflow/index.html",
    "href": "posts/workflow/index.html",
    "title": "What is Workflow Anyway?",
    "section": "",
    "text": "A data science workflow is a structured, repeatable process for turning raw data into actionable insights, covering everything from data collection to communication. It ensures your work is organized, reproducible, and scalable. It is how you approach a defined task and preferably you are going to do it the same way every time. Although you may want to tweak and improve your way of doing things as circumstances dictate.\nIf you‚Äôre going to be writing a lot of R code, or otherwise doing a lot of data analysis, it‚Äôs worth investing some time to plan and edit your basic workflow. This is because it tends to pay great [dividends](https://www.datascience-pm.com/data-science-workflow/#:~:text=Using%20a%20well%2Ddefined%20data%20science%20workflow%20is,done%20to%20do%20a%20data%20science%20project.&text=Each%20phase%20(Business%20understanding%2C%20data%20understanding%2C%20data,of%20deliverables%20such%20as%20documentation%20and%20reports.) in the long run. It doesn‚Äôt just increase the proportion of your time spent writing code, but because you see the results more quickly, it makes the process of writing code more enjoyable, and helps your skills improve more quickly.\nThink of it as a way to get immediate feedback on your deliberate practice.\nAre you sold on the idea of at least having a rough outline of how you perform routine data analysis tasks? Great! Let‚Äôs gooo‚Ä¶. I like to break nine well-known steps down into groups of three. Three phases of three, if that helps.\n\nPlan, collect and clean\nExplore, model and validate\nCommunicate, reproduce and deploy"
  },
  {
    "objectID": "posts/workflow/index.html#a-software-development-term-that-is-often-used-and-just-as-often-misunderstood.",
    "href": "posts/workflow/index.html#a-software-development-term-that-is-often-used-and-just-as-often-misunderstood.",
    "title": "What is Workflow Anyway?",
    "section": "",
    "text": "A data science workflow is a structured, repeatable process for turning raw data into actionable insights, covering everything from data collection to communication. It ensures your work is organized, reproducible, and scalable. It is how you approach a defined task and preferably you are going to do it the same way every time. Although you may want to tweak and improve your way of doing things as circumstances dictate.\nIf you‚Äôre going to be writing a lot of R code, or otherwise doing a lot of data analysis, it‚Äôs worth investing some time to plan and edit your basic workflow. This is because it tends to pay great [dividends](https://www.datascience-pm.com/data-science-workflow/#:~:text=Using%20a%20well%2Ddefined%20data%20science%20workflow%20is,done%20to%20do%20a%20data%20science%20project.&text=Each%20phase%20(Business%20understanding%2C%20data%20understanding%2C%20data,of%20deliverables%20such%20as%20documentation%20and%20reports.) in the long run. It doesn‚Äôt just increase the proportion of your time spent writing code, but because you see the results more quickly, it makes the process of writing code more enjoyable, and helps your skills improve more quickly.\nThink of it as a way to get immediate feedback on your deliberate practice.\nAre you sold on the idea of at least having a rough outline of how you perform routine data analysis tasks? Great! Let‚Äôs gooo‚Ä¶. I like to break nine well-known steps down into groups of three. Three phases of three, if that helps.\n\nPlan, collect and clean\nExplore, model and validate\nCommunicate, reproduce and deploy"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Build a Data Science Business",
    "section": "",
    "text": "At some point you have dedicated yourself to accumulating knowledge and helping others in the field of data science. You may not have realized it, but you are laying the foundation to have your own business. In addition to a traditional corporate gig you want to build a data science business. This can be profitable, educational, and believe it or not, fun. Think about it. You can pursue professional relationships that are actually meaningful to you.\n\n\nYou can solve problems and help people. Not just any people, but the people you choose to work with. You get to boldly display all of these technical skills that you have been building for all these years.\nIf you are diligent, you will at least generate enough cash flow to have some flexibility in your life. If you are careful with your budget and stash some cash away in investments, you can actually get to financial independence more quickly than your peers.\n\n\n\nWhen you build a business, it means to take a risk in terms of time and money to create something of value. It is not a hobby. Hobbies don‚Äôt create a profit. At some point, others need to recognize this value and present you with money (society‚Äôs IOU for work done) that exceeds the value of the amount of time and money you risked in the first place. When your value creation process is readily reproducible, you have a business. It means an independent source of income based upon ownership of the means of production. It could mean freelancing on Fivver and/or Upwork.\nI think developing a small business based on technical skills is the smart way to hustle. This requires the curiosity to purposefully learn new things. Not everyone has that, and you have to do a gut check to ensure that you do. My gut check was when I attempted side hustles such as Uber Eats and even Air BnB with varying degrees of success. What I didn‚Äôt like about those apps is that almost anyone can do the tasks assigned. That sounds great from an egalitarian standpoint but not from a capitalist standpoint. That means that competition is fundamentally stiff and the wages are literally a ‚Äúrace to the bottom.‚Äù Who is going to do the simple task at the lowest price? OK, you, desperate person, are the lucky recipient of less than minimum wage then. There are some success stories, and I can totally understand even a handyman with some practical skills could do passably well on something like Task Rabbit.\nHowever, let‚Äôs say you don‚Äôt mind being in front of a screen for a few hours each day and actually enjoy solving puzzles to the point of distraction. You constantly want to learn the right way to do things. This is the essence of the grit that it takes.\nData science is particularly fascinating because of its component disciplines. These include data collection, data wrangling, data visualization, statistics, calculus, linear algebra, computer science. If any of these fields hold any interest for you at all then you are squarely in the right place.\n\n\n\nOK, so we thought about why we are pursuing data science, we named a lot of what data science consists of, but what is data science as a whole? Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data, and apply knowledge and actionable insights from data across a broad range of application domains.\nThat‚Äôs it. It is not even restrictive or exclusive to a particular domain, such as nursing or cattle ranching. It could be both or neither of those. That is the beauty of building these analytical skills. The skills can be applied anywhere and almost every field of study, work or dreams which benefit from data. We see it over and over in entertainment, construction, medicine, and even gambling.\nA friend asked me recently how the odds could be placed so precisely and game scores could be so accurately predicted. My answer was that data automates and makes accessible what experts in the subject, like Ace Rothstein in the movie ‚ÄòCasino‚Äô used to do. It enables prediction, categorization and regression and can improve any endeavor if used properly. What this means is if you are already savvy about woodworking, international sales, small business operations or whatever then you can combine data science skills with knowledge you already have in order to have your own niche.\nHere is a rough outline for what a modern, branded data science content creator could do:\n-make a bad video for YouTube\n-make more videos for YouTube, getting better each time\n-blog for Medium, using that to cross promote your YouTube channel\n-get to 10,000 subscribers\n-set up a Patreon account\n-monetize with advertisers and get affiliate links\n-follow up with Google adsense\n-write an eBook related to learning data science and cross promote it on YouTube and Medium\n-start and improve a dedicated, branded Facebook page\n-start and improve an Instagram page\n-cross blog and write content for others to promote your brand\n-teach data science online and improve your teaching profile on specific educational and freelancing sites and use it to promote your channel\n-get a logo done and promote merchandise with dropshipping\n-build a great course and sell it, acquiring the first 100 customers\n-work like hell to impress those 100 people and one out of ten will buy your stuff and market your services for free through reviews, social media posts, etc."
  },
  {
    "objectID": "posts/welcome/index.html#the-ideal",
    "href": "posts/welcome/index.html#the-ideal",
    "title": "Build a Data Science Business",
    "section": "",
    "text": "You can solve problems and help people. Not just any people, but the people you choose to work with. You get to boldly display all of these technical skills that you have been building for all these years.\nIf you are diligent, you will at least generate enough cash flow to have some flexibility in your life. If you are careful with your budget and stash some cash away in investments, you can actually get to financial independence more quickly than your peers."
  },
  {
    "objectID": "posts/welcome/index.html#the-reality",
    "href": "posts/welcome/index.html#the-reality",
    "title": "Build a Data Science Business",
    "section": "",
    "text": "When you build a business, it means to take a risk in terms of time and money to create something of value. It is not a hobby. Hobbies don‚Äôt create a profit. At some point, others need to recognize this value and present you with money (society‚Äôs IOU for work done) that exceeds the value of the amount of time and money you risked in the first place. When your value creation process is readily reproducible, you have a business. It means an independent source of income based upon ownership of the means of production. It could mean freelancing on Fivver and/or Upwork.\nI think developing a small business based on technical skills is the smart way to hustle. This requires the curiosity to purposefully learn new things. Not everyone has that, and you have to do a gut check to ensure that you do. My gut check was when I attempted side hustles such as Uber Eats and even Air BnB with varying degrees of success. What I didn‚Äôt like about those apps is that almost anyone can do the tasks assigned. That sounds great from an egalitarian standpoint but not from a capitalist standpoint. That means that competition is fundamentally stiff and the wages are literally a ‚Äúrace to the bottom.‚Äù Who is going to do the simple task at the lowest price? OK, you, desperate person, are the lucky recipient of less than minimum wage then. There are some success stories, and I can totally understand even a handyman with some practical skills could do passably well on something like Task Rabbit.\nHowever, let‚Äôs say you don‚Äôt mind being in front of a screen for a few hours each day and actually enjoy solving puzzles to the point of distraction. You constantly want to learn the right way to do things. This is the essence of the grit that it takes.\nData science is particularly fascinating because of its component disciplines. These include data collection, data wrangling, data visualization, statistics, calculus, linear algebra, computer science. If any of these fields hold any interest for you at all then you are squarely in the right place."
  },
  {
    "objectID": "posts/welcome/index.html#where-do-we-go-now",
    "href": "posts/welcome/index.html#where-do-we-go-now",
    "title": "Build a Data Science Business",
    "section": "",
    "text": "OK, so we thought about why we are pursuing data science, we named a lot of what data science consists of, but what is data science as a whole? Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data, and apply knowledge and actionable insights from data across a broad range of application domains.\nThat‚Äôs it. It is not even restrictive or exclusive to a particular domain, such as nursing or cattle ranching. It could be both or neither of those. That is the beauty of building these analytical skills. The skills can be applied anywhere and almost every field of study, work or dreams which benefit from data. We see it over and over in entertainment, construction, medicine, and even gambling.\nA friend asked me recently how the odds could be placed so precisely and game scores could be so accurately predicted. My answer was that data automates and makes accessible what experts in the subject, like Ace Rothstein in the movie ‚ÄòCasino‚Äô used to do. It enables prediction, categorization and regression and can improve any endeavor if used properly. What this means is if you are already savvy about woodworking, international sales, small business operations or whatever then you can combine data science skills with knowledge you already have in order to have your own niche.\nHere is a rough outline for what a modern, branded data science content creator could do:\n-make a bad video for YouTube\n-make more videos for YouTube, getting better each time\n-blog for Medium, using that to cross promote your YouTube channel\n-get to 10,000 subscribers\n-set up a Patreon account\n-monetize with advertisers and get affiliate links\n-follow up with Google adsense\n-write an eBook related to learning data science and cross promote it on YouTube and Medium\n-start and improve a dedicated, branded Facebook page\n-start and improve an Instagram page\n-cross blog and write content for others to promote your brand\n-teach data science online and improve your teaching profile on specific educational and freelancing sites and use it to promote your channel\n-get a logo done and promote merchandise with dropshipping\n-build a great course and sell it, acquiring the first 100 customers\n-work like hell to impress those 100 people and one out of ten will buy your stuff and market your services for free through reviews, social media posts, etc."
  },
  {
    "objectID": "posts/tidyverse_rocks/index.html",
    "href": "posts/tidyverse_rocks/index.html",
    "title": "The Tidyverse Rocks",
    "section": "",
    "text": "The Tidyverse Rocks\n\nChoosing R packages to help your coding is subjective and depends greatly on your specific needs and field of study. However, some packages stand out for their versatility, impact, and community support. Here, I‚Äôll delve into tidyverse, a collection of packages that revolutionized data manipulation and analysis in R, focusing on its functionalities and practical applications with code examples.\ntidyverse ‚Äî A Powerhouse for Data Science\nDeveloped by Hadley Wickham and others, tidyverse is a collection of interoperable packages promoting a consistent and readable workflow for data analysis. It emphasizes tidy data principles, encouraging data organization into rectangular tables with well-defined variables and observations. This simplifies tasks like aggregation, transformation, and visualization while fostering reproducibility and collaboration.\nKey Components:\n\ndplyr: The heart of data manipulation, providing verbs like filter, mutate, and summarize for efficient data wrangling.\nggplot2: A powerful and flexible grammar for creating elegant and informative visualizations.\nreadr: Reads data from various file formats, handling different encodings and delimiters.\npurrr: Offers functions for functional programming, enabling vectorized operations and applying functions over groups of data.\ntibble: Defines a data frame class optimized for tidiness and efficiency.\nComplementary packages: stringr, lubridate, forcats, etc., extend functionalities for specific tasks.\n\nPractical Applications:\nNote how tidyverse functions are key parts of all of these processes. Could you install and load each package separately in order to save memory? Sure, but that concern probably does not apply to most of us.\n\nExploratory Data Analysis:\n\n# Load libraries library(dplyr) library(ggplot2)  # Load data (example dataset) data(iris)  # Explore specific species iris %&gt;%   filter(Species == \"virginica\") %&gt;%   summarize(mean_sepal_length = mean(Sepal.Length),              sd_sepal_length = sd(Sepal.Length))  # Visualize petal distribution ggplot(iris, aes(x = Petal.Length, y = density)) +   geom_density(fill = \"lightblue\") +   facet_wrap(~ Species)\n2. Feature Engineering:\n# Create new features df &lt;- df %&gt;%   mutate(petal_area = Sepal.Width * Sepal.Length) %&gt;%   mutate(petal_aspect_ratio = Petal.Length / Petal.Width)\n3. Statistical Modeling:\n# Load the `lm` package library(lm)  # Fit a linear model model &lt;- lm(Sepal.Length ~ Species, data = iris)  # Summarize model results summary(model)\n4. Data Visualization:\n# Create a scatter plot with trendline ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) +   geom_point() +   geom_smooth(method = \"lm\") +   facet_wrap(~ Species)\n5. Data Reporting:\n# Generate descriptive statistics library(kableExtra)  kable(iris %&gt;%   group_by(Species) %&gt;%   summarize(mean_sepal_length = mean(Sepal.Length),              sd_sepal_length = sd(Sepal.Length)))\nBeyond the Code:\n\nLearning Resources: Abundant tutorials, books, and courses make learning tidyverse easy.\nCommunity Support: A vibrant community provides active discussions, solutions, and package development.\nInteroperability: Integrates seamlessly with other R packages for diverse workflows.\n\n\n\ntldr; Conclusion\nWhile selecting one package to improve your code can be a subjective task, selecting tidyverse for its versatility is an undeniable advantage. Its philosophy, functionality, and active community make it a powerful tool for beginners and experts alike. By exploring its components and their applications, you can harness its strengths for efficient and reproducible data analysis."
  },
  {
    "objectID": "posts/linear-regression-with-code/index.html",
    "href": "posts/linear-regression-with-code/index.html",
    "title": "Linear Regression for Gentle Souls",
    "section": "",
    "text": "Linear Regression in R for Gentle Souls\nIn statistics, linear regression is a linear approach for modelling the relationship between a response and one or more explanatory variables. It is fairly intuitive and works fairly well in the cases where a linear relationship exists between variables.\nLinear regression is a statistical technique used to model the relationship between a dependent variable (also known as the response variable) and one or more independent variables (also known as predictor variables). The goal of linear regression is to find the best linear relationship between the dependent variable and the independent variables.\nIn simple linear regression, there is only one independent variable. The relationship between the dependent variable and the independent variable can be represented by a straight line, hence the term ‚Äúlinear‚Äù regression. The equation for a simple linear regression model is:\nY = a + bX + e\nWhere:\n\nY is the dependent variable\nX is the independent variable\na is the intercept (the value of Y when X=0)\nb is the slope (the change in Y for a one-unit change in X)\ne is the error term (the difference between the predicted value of Y and the actual value of Y)\n\nIn multiple linear regression, there are multiple independent variables. The equation for a multiple linear regression model is similar to the simple linear regression model, but with multiple independent variables:\nY = a + b1X1 + b2X2 + ‚Ä¶ + bnXn + e\nWhere:\n\nY is the dependent variable\nX1, X2, ‚Ä¶, Xn are the independent variables\na is the intercept\nb1, b2, ‚Ä¶, bn are the slopes\ne is the error term\n\nTo build a linear regression model in R, you can use the lm() function. Here‚Äôs an example:\n\n# Load necessary libraries\ninstall.packages('ggplot2')\n\nInstalling package into '/cloud/lib/x86_64-pc-linux-gnu-library/4.4'\n(as 'lib' is unspecified)\n\nlibrary(ggplot2)\n\n# Load the iris dataset\ndata(iris)\n\n# Fit a simple linear regression model\nmodel &lt;- lm(Petal.Width ~ Petal.Length, data = iris)\n\n# Show the model summary\nsummary(model)\n\n\nCall:\nlm(formula = Petal.Width ~ Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.56515 -0.12358 -0.01898  0.13288  0.64272 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.363076   0.039762  -9.131  4.7e-16 ***\nPetal.Length  0.415755   0.009582  43.387  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2065 on 148 degrees of freedom\nMultiple R-squared:  0.9271,    Adjusted R-squared:  0.9266 \nF-statistic:  1882 on 1 and 148 DF,  p-value: &lt; 2.2e-16\n\n# Plot with regression line\nggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +\n  geom_point(color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Simple Linear Regression: Petal.Length vs Petal.Width\",\n       x = \"Petal Length\",\n       y = \"Petal Width\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe first load the data and build a simple linear regression model using the¬†lm()¬†function. We then view the summary of the model to see the coefficients, R-squared value, and other statistics. We can then make predictions using the model by creating a new data frame with the independent variable values we want to predict for, and using the¬†predict()¬†function to generate the predicted values.\n\n\nFinal Thought\nBe advised that a linear relationship does not always exist between two variables, yet they are related. Linear regression is just one tool of many that can be utilized to describe the reality of your data."
  },
  {
    "objectID": "posts/new-and-improved-R-4.5/index.html",
    "href": "posts/new-and-improved-R-4.5/index.html",
    "title": "A New and Improved R: Cool Use Cases of R 4.5.0 Upgrades",
    "section": "",
    "text": "R 4.5 (released April 2025) brings performance, memory, and developer usability enhancements. Cool use cases include faster large data manipulation, enhanced reproducibility via random number generation, and better developer experience through enriched C-level introspection and string handling. It benefits tidyverse-heavy workflows, high-performance simulations, and package development.\nHere‚Äôs a breakdown of some cool use cases unlocked or improved by R 4.5 and how you can apply them in real-world workflows:"
  },
  {
    "objectID": "posts/new-and-improved-R-4.5/index.html#r-makes-you-money-and-saves-you-time.-why-not-upgrade-your-r-game",
    "href": "posts/new-and-improved-R-4.5/index.html#r-makes-you-money-and-saves-you-time.-why-not-upgrade-your-r-game",
    "title": "A New and Improved R: Cool Use Cases of R 4.5.0 Upgrades",
    "section": "",
    "text": "R 4.5 (released April 2025) brings performance, memory, and developer usability enhancements. Cool use cases include faster large data manipulation, enhanced reproducibility via random number generation, and better developer experience through enriched C-level introspection and string handling. It benefits tidyverse-heavy workflows, high-performance simulations, and package development.\nHere‚Äôs a breakdown of some cool use cases unlocked or improved by R 4.5 and how you can apply them in real-world workflows:"
  },
  {
    "objectID": "posts/new-and-improved-R-4.5/index.html#upgrade",
    "href": "posts/new-and-improved-R-4.5/index.html#upgrade",
    "title": "A New and Improved R: Cool Use Cases of R 4.5.0 Upgrades",
    "section": "Upgrade:",
    "text": "Upgrade:\n\nR 4.5 enhances random number generation (RNG) seeding and reproducibility in parallel environments (like future, parallel, foreach)."
  },
  {
    "objectID": "posts/new-and-improved-R-4.5/index.html#use-case",
    "href": "posts/new-and-improved-R-4.5/index.html#use-case",
    "title": "A New and Improved R: Cool Use Cases of R 4.5.0 Upgrades",
    "section": "Use case:",
    "text": "Use case:\n\nTry running thousands of Monte Carlo simulations or cross-validations in parallel using {furrr} or {doParallel} with consistent RNG across sessions.\n\nset.seed(42, kind = \"L'Ecuyer-CMRG\")  # Better for parallel RNG"
  },
  {
    "objectID": "posts/new-and-improved-R-4.5/index.html#upgrade-1",
    "href": "posts/new-and-improved-R-4.5/index.html#upgrade-1",
    "title": "A New and Improved R: Cool Use Cases of R 4.5.0 Upgrades",
    "section": "Upgrade:",
    "text": "Upgrade:\n\nThere are speed gains in low-level vector operations and memory usage, thanks to tighter internal memory allocations and improved hashing."
  },
  {
    "objectID": "posts/new-and-improved-R-4.5/index.html#use-case-1",
    "href": "posts/new-and-improved-R-4.5/index.html#use-case-1",
    "title": "A New and Improved R: Cool Use Cases of R 4.5.0 Upgrades",
    "section": "Use case:",
    "text": "Use case:\n\nLarge tidyverse data pipelines now run faster with less RAM overhead, especially when chaining dplyr and purrr operations on big datasets (e.g., millions of rows).\n\nlibrary(dplyr) big_data %&gt;%   group_by(group_var) %&gt;%   summarize(mean_val = mean(value), .groups = 'drop')"
  },
  {
    "objectID": "posts/new-and-improved-R-4.5/index.html#upgrade-2",
    "href": "posts/new-and-improved-R-4.5/index.html#upgrade-2",
    "title": "A New and Improved R: Cool Use Cases of R 4.5.0 Upgrades",
    "section": "Upgrade:",
    "text": "Upgrade:\n\nNow, new and improved (!) nchar(), substr(), and string encoding support across OSes."
  },
  {
    "objectID": "posts/new-and-improved-R-4.5/index.html#use-case-2",
    "href": "posts/new-and-improved-R-4.5/index.html#use-case-2",
    "title": "A New and Improved R: Cool Use Cases of R 4.5.0 Upgrades",
    "section": "Use case:",
    "text": "Use case:\n\nText mining and NLP tasks (especially on multilingual data) using {stringr} or {stringi} have fewer encoding bugs and cleaner Unicode handling.\n\nlibrary(stringr) str_length(\"üòä‰Ω†Â•Ω\")  # Accurate across locales"
  },
  {
    "objectID": "posts/new-and-improved-R-4.5/index.html#upgrade-3",
    "href": "posts/new-and-improved-R-4.5/index.html#upgrade-3",
    "title": "A New and Improved R: Cool Use Cases of R 4.5.0 Upgrades",
    "section": "Upgrade:",
    "text": "Upgrade:\n\nNew API for internal function calls and better support for C/C++-based extensions. C++ is used for everything from web browsers to robotic automation."
  },
  {
    "objectID": "posts/new-and-improved-R-4.5/index.html#use-case-3",
    "href": "posts/new-and-improved-R-4.5/index.html#use-case-3",
    "title": "A New and Improved R: Cool Use Cases of R 4.5.0 Upgrades",
    "section": "Use case:",
    "text": "Use case:\n\nFor developers writing high-performance R packages in C++, integration is smoother, safer, and more debuggable (great for {Rcpp} devs).\n\n// Better .Call() introspection from R side"
  },
  {
    "objectID": "posts/new-and-improved-R-4.5/index.html#upgrade-4",
    "href": "posts/new-and-improved-R-4.5/index.html#upgrade-4",
    "title": "A New and Improved R: Cool Use Cases of R 4.5.0 Upgrades",
    "section": "Upgrade:",
    "text": "Upgrade:\n\nBetter stack traces and error source tracking in base R."
  },
  {
    "objectID": "posts/new-and-improved-R-4.5/index.html#use-case-4",
    "href": "posts/new-and-improved-R-4.5/index.html#use-case-4",
    "title": "A New and Improved R: Cool Use Cases of R 4.5.0 Upgrades",
    "section": "Use case:",
    "text": "Use case:\n\nEasier debugging of deeply nested tidyverse or {targets} workflows.\n\noptions(error = utils::recover)  # Now shows better tracebacks"
  },
  {
    "objectID": "posts/new-and-improved-R-4.5/index.html#upgrade-5",
    "href": "posts/new-and-improved-R-4.5/index.html#upgrade-5",
    "title": "A New and Improved R: Cool Use Cases of R 4.5.0 Upgrades",
    "section": "Upgrade:",
    "text": "Upgrade:\n\nImproved forking behavior, lower memory duplication."
  },
  {
    "objectID": "posts/new-and-improved-R-4.5/index.html#use-case-5",
    "href": "posts/new-and-improved-R-4.5/index.html#use-case-5",
    "title": "A New and Improved R: Cool Use Cases of R 4.5.0 Upgrades",
    "section": "Use case:",
    "text": "Use case:\n\nParallel backends like {parallel}, {future}, {multidplyr} now waste less RAM when duplicating large objects across workers."
  },
  {
    "objectID": "posts/new-and-improved-R-4.5/index.html#upgrade-6",
    "href": "posts/new-and-improved-R-4.5/index.html#upgrade-6",
    "title": "A New and Improved R: Cool Use Cases of R 4.5.0 Upgrades",
    "section": "Upgrade:",
    "text": "Upgrade:\n\n|&gt; gains improved handling of side-effects and evaluation."
  },
  {
    "objectID": "posts/new-and-improved-R-4.5/index.html#use-case-6",
    "href": "posts/new-and-improved-R-4.5/index.html#use-case-6",
    "title": "A New and Improved R: Cool Use Cases of R 4.5.0 Upgrades",
    "section": "Use case:",
    "text": "Use case:\n\nYou can now safely replace many %&gt;% chains with |&gt; in lighter base workflows.\n\niris |&gt;   subset(Species == \"setosa\") |&gt;   head()\nIf you share your current use case or type of work (e.g., modeling, dashboards, NLP, simulations) in the comments, then I can show you how R 4.5 specifically makes your code cleaner, faster, or safer.\nNick Tierney provides more details about R 4.5.0 here in his R blog. Overall, this is exciting news and likely not the last article I write on the fantastic advancements being made in R."
  },
  {
    "objectID": "posts/change-columns/index.html#take-the-data-mutate-it-by-naming-a-new-column-then-define-how-its-changed.",
    "href": "posts/change-columns/index.html#take-the-data-mutate-it-by-naming-a-new-column-then-define-how-its-changed.",
    "title": "Easily Create and Change Columns in Your Data",
    "section": "‚ÄúTake the data, mutate it by naming a new column, then define how it‚Äôs changed.‚Äù",
    "text": "‚ÄúTake the data, mutate it by naming a new column, then define how it‚Äôs changed.‚Äù\npenguins %&gt;%   mutate(body_mass_kg = body_mass_g / 1000)\n\npenguins is your data\nbody_mass_kg is the new column\n= body_mass_g / 1000 is how to create it\n\n\nD-N-E-C: ‚ÄúDon‚Äôt Neglect Essential Columns‚Äù\n\nWhere D means Data penguins %&gt;% , and N means New column name, body_mass_kg , E is Equals = or &lt;- and C is Change (transformation) body_mass_g / 1000"
  },
  {
    "objectID": "posts/another post-with-code/index.html",
    "href": "posts/another post-with-code/index.html",
    "title": "Peak Shiny Performance",
    "section": "",
    "text": "Professional software developers think about not only initial execution but performance optimization. It is part of your journey of being one of the best in your field. There are several ways to do this for an app. Some of these ways include: minimizing the number of computations, using efficient data structures and algorithms, properly caching your data, and optimizing image and file sizes. This article touches on these subjects as they relate to Shiny, one of the best web app development tools available.\nR Shiny has a certain amount of popularity for building interactive web applications, especially in the data science and analytics space. Therefore, developers constantly face the challenge of scaling and optimizing performance. While Shiny is intuitive and productive for creating prototypes, deploying apps for broader audiences or handling large datasets demands a more deliberate focus on performance tuning.\nLet‚Äôs explore key techniques and best practices that web developers can use to achieve peak performance with Shiny apps, covering front-end and back-end considerations, efficient coding strategies, and deployment tips."
  },
  {
    "objectID": "posts/another post-with-code/index.html#dont-just-make-apps-that-work.-make-apps-that-work-better.",
    "href": "posts/another post-with-code/index.html#dont-just-make-apps-that-work.-make-apps-that-work-better.",
    "title": "Peak Shiny Performance",
    "section": "",
    "text": "Professional software developers think about not only initial execution but performance optimization. It is part of your journey of being one of the best in your field. There are several ways to do this for an app. Some of these ways include: minimizing the number of computations, using efficient data structures and algorithms, properly caching your data, and optimizing image and file sizes. This article touches on these subjects as they relate to Shiny, one of the best web app development tools available.\nR Shiny has a certain amount of popularity for building interactive web applications, especially in the data science and analytics space. Therefore, developers constantly face the challenge of scaling and optimizing performance. While Shiny is intuitive and productive for creating prototypes, deploying apps for broader audiences or handling large datasets demands a more deliberate focus on performance tuning.\nLet‚Äôs explore key techniques and best practices that web developers can use to achieve peak performance with Shiny apps, covering front-end and back-end considerations, efficient coding strategies, and deployment tips."
  },
  {
    "objectID": "posts/complex.v.complicated/index.html",
    "href": "posts/complex.v.complicated/index.html",
    "title": "What is Complexity? Well, It Is Not Complicated.",
    "section": "",
    "text": "Teamwork can often lead to success solving both complicated and complex problems.\n\n\nIn a world filled with challenges, knowing the difference between what is ‚Äúcomplicated‚Äù and what is ‚Äúcomplex‚Äù can be the key to solving problems more effectively. While the two words are often used interchangeably, they describe very different realities. As you approach dilemmas, you want to use the correct models to solve them. If you can define what you face as either ‚Äúcomplicated‚Äù or ‚Äúcomplex‚Äù then that is a positive step towards defining the problem as a whole.\nComplicated systems ‚Äî like a software program‚Äî may be intricate but can be broken down, analyzed, repaired and improved. Complex systems ‚Äî like climate patterns or human relationships ‚Äî are dynamic, with several factors and unpredictable outcomes. Recognizing whether you‚Äôre dealing with a complicated task or a complex situation can shape your thinking. This recognition alerts to as to whether you need a manual (‚Äúcomplicated‚Äù), or a mindset (‚Äúcomplex‚Äù) that is ready to adapt and evolve."
  },
  {
    "objectID": "posts/complex.v.complicated/index.html#while-complex-and-complicated-are-often-used-interchangeably-the-differences-can-have-profound-implications-for-your-life.",
    "href": "posts/complex.v.complicated/index.html#while-complex-and-complicated-are-often-used-interchangeably-the-differences-can-have-profound-implications-for-your-life.",
    "title": "What is Complexity? Well, It Is Not Complicated.",
    "section": "",
    "text": "Teamwork can often lead to success solving both complicated and complex problems.\n\n\nIn a world filled with challenges, knowing the difference between what is ‚Äúcomplicated‚Äù and what is ‚Äúcomplex‚Äù can be the key to solving problems more effectively. While the two words are often used interchangeably, they describe very different realities. As you approach dilemmas, you want to use the correct models to solve them. If you can define what you face as either ‚Äúcomplicated‚Äù or ‚Äúcomplex‚Äù then that is a positive step towards defining the problem as a whole.\nComplicated systems ‚Äî like a software program‚Äî may be intricate but can be broken down, analyzed, repaired and improved. Complex systems ‚Äî like climate patterns or human relationships ‚Äî are dynamic, with several factors and unpredictable outcomes. Recognizing whether you‚Äôre dealing with a complicated task or a complex situation can shape your thinking. This recognition alerts to as to whether you need a manual (‚Äúcomplicated‚Äù), or a mindset (‚Äúcomplex‚Äù) that is ready to adapt and evolve."
  },
  {
    "objectID": "posts/complex.v.complicated/index.html#technology-building-vs.-managing",
    "href": "posts/complex.v.complicated/index.html#technology-building-vs.-managing",
    "title": "What is Complexity? Well, It Is Not Complicated.",
    "section": "Technology: Building vs.¬†Managing",
    "text": "Technology: Building vs.¬†Managing\nConstructing a smartphone is a complicated task. It involves assembling thousands of components with precision. But there‚Äôs a blueprint, and with the right tools and expertise, the process is manageable and repeatable.\nContrast that with running a social media platform. This is complex. User behavior, algorithm design, viral content, and misinformation all interact in unpredictable ways. What works one day may backfire the next, and no single fix can guarantee success. It‚Äôs a system in constant flux, requiring adaptation and learning rather than strict control."
  },
  {
    "objectID": "posts/complex.v.complicated/index.html#biology-structures-vs.-systems",
    "href": "posts/complex.v.complicated/index.html#biology-structures-vs.-systems",
    "title": "What is Complexity? Well, It Is Not Complicated.",
    "section": "Biology: Structures vs.¬†Systems",
    "text": "Biology: Structures vs.¬†Systems\nThe human skeletal system is an example of a complicated biological structure. It can be mapped, understood, and repaired using established knowledge and tools.\nThe immune system, however, is complex. It doesn‚Äôt just respond to threats in a linear fashion ‚Äî it adapts, remembers, and sometimes misfires. Interactions with diet, stress, pathogens, and genetics create a living system that‚Äôs difficult to predict and even harder to fully master."
  },
  {
    "objectID": "posts/complex.v.complicated/index.html#human-behavior-tasks-vs.-relationships",
    "href": "posts/complex.v.complicated/index.html#human-behavior-tasks-vs.-relationships",
    "title": "What is Complexity? Well, It Is Not Complicated.",
    "section": "Human Behavior: Tasks vs.¬†Relationships",
    "text": "Human Behavior: Tasks vs.¬†Relationships\nFiling your taxes may be a pain, but it‚Äôs complicated, not complex. The forms and rules are detailed, but with enough effort ‚Äî or a good accountant ‚Äî you can get through it.\nRaising a teenager, on the other hand, is complex. Emotions, peer influence, development stages, and communication all mix in unique ways for each child. There‚Äôs no manual, and what works one day might fail the next. It‚Äôs an evolving relationship that demands empathy, flexibility, and patience."
  },
  {
    "objectID": "posts/CopyOfCopyOfpost-with-code/index.html",
    "href": "posts/CopyOfCopyOfpost-with-code/index.html",
    "title": "Dad Bod Be Gone",
    "section": "",
    "text": "Dad Bod Be Gone\nFathers have a unique and important role in their children‚Äôs lives. Not only do they provide emotional and financial support, but fathers also have a unique opportunity to pass on physical fitness habits to their children.\nI had this in mind as I asked a friend about what makes for a good diet and fitness plan. These two things are inextricably linked. There is a reason that you constantly hear the phrase, ‚Äúdiet and exercise.‚Äù You simply can‚Äôt work out like a madman and come home to eat an entire pizza. You have to consider the long-term consequences of both because one affects the other.\nGoogling or otherwise internet searching yields a nightmare of advertisements and dubious information that you still have to sift through. ChatGPT is getting there, and worth a spin. However, I had a source close to home.\nLuckily for us, my friend is both a fitness trainer and a trained chef.\nThis is what he recommended:\n\nStop drinking alcohol. ‚ÄúSorry Bro‚Äù were his exact words. I know some people will already be flipping out but being more discriminating about when and what you drink will only increase your appreciation for good alcohol.\nSensible carbohydrate intake. Your mainstays are going to be greener and not the pasta and bread that you eat now.\nPlan your protein intake. Moderate amounts are fine.\nGood fats are largely ok.\nNix any bad oils.\nIntermittent fasting works.\nRegular zone cardio exercise.\nResistance training, and it doesn‚Äôt necessarily require any equipment.\n\n‚ÄúI prefer not to drink and I think it directly contributed to me seeing my ab muscles for the first time in my life, as has the rest of the list. That being said, some studies have suggest that small amounts of alcohol may have health benefits for some people, such as lowering blood pressure and the risk of blood clots. You can drink, but sparingly.\nA low carbohydrate diet with moderate protein and healthy fats can be an effective way to manage weight and improve overall health. To execute this type of diet, you can focus on consuming whole, unprocessed foods such as vegetables, nuts, seeds, and healthy fats like avocado, olive oil, and coconut oil. You can also include moderate amounts of protein sources like eggs, chicken, fish, and tofu. It is important to avoid bad oils like vegetable oil, canola oil, and margarine, and instead opt for healthier options like olive oil and avocado oil.\nIntermittent fasting is a dietary pattern that involves alternating periods of eating and fasting. One way to incorporate intermittent fasting into your diet is to do a 16/8 split, where you fast for 16 hours and then eat during an 8-hour window. For example, you could stop eating at 8pm and then not eat again until noon the next day. This can be a convenient way to structure your eating habits and can also help with weight management.\nIn terms of fitness, incorporating zone cardiovascular exercise can be a great way to improve your cardiovascular health and burn calories. Zone cardiovascular exercise involves working at a specific heart rate zone, typically between 50‚Äì85% of your maximum heart rate, in order to achieve specific training goals. This can be done through activities like running, cycling, swimming, or using cardio machines like treadmills or stationary bikes.\nIn addition to zone cardiovascular exercise, it is also important to incorporate resistance training into your fitness routine. Resistance training can help build and maintain muscle mass, improve bone density, and increase metabolism. All of these things are very important as we grow older. Aim to do three to five days of resistance training per week, using a variety of exercises that target different muscle groups. Some examples of resistance training exercises include squats, lunges, push-ups, rows, and bicep curls and even running in the pool. I am a fan of kettle bells because of the range of movement and number of exercises possible. Dumbbells are also a good option because you can more or less isolate muscles on one side of your body. It is important to start with a weight that is manageable and that you gradually increase the weight as you become stronger.‚Äù\nOverall, a diet that is low in carbohydrates and incorporates intermittent fasting, combined with zone cardiovascular exercise and resistance training, can be an effective way to manage weight, improve overall health, and achieve your fitness goals.\n\n\nFinal Thoughts\nThat‚Äôs it, aside from adopting the right mindset. Stay positive. Don‚Äôt buy anything new other than more green, leafy food. Be consistent and track your progress in a free tool like Google Calendar. Or even just a pencil and that free paper calendar that you got from your local bank, gas station, or in my case the public works department of the City of Baltimore. If you stray from the path, be kind to yourself and guide yourself back to the path as soon as possible. Find others who can share your diet and exercise plan. Find joy in them and the process. If it all seems overwhelming then just breathe, order a small low-carb Thai food takeout dish and enjoy."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "The Double-Edged Code: How Data Became Both Our Salvation and Our Undoing",
    "section": "",
    "text": "Data is beautiful and data is terrible. It is simultaneously helpful and dangerous. It is a construct that promotes progress, yet also can be the source of all human oblivion. Data binds and emulsifies modern society, and still is capable of crushing an individual and creating a dystopian future. How can these contradictions be possible?\nA good place to start about how to understand data and its contradictions is to define it. Data is a tally or score. Technically plural, they are facts and statistics. Often it is expressed in the singular without significantly distorting the concept. A data set is many pieces of data clumped together, yet it is a whole. Data can be as simple as a count; such as the number of times you tap your foot at a book club meeting. It can also be as complex as the number of human interactions on earth in a given day.\nData has always existed, it is just how humans think about it that has changed. There are actually an infinite number of observable actions, moments and counts throughout the universe, let alone just on earth, and many were basically mystical and partly meaningless to all life on earth, including humankind, for millennia. There were a finite number of species, several different global extinctions, and an average amount of photosynthesis that took place on Pangea. However, most of it was effectively ignored. This is especially true of the smallest units or largest units of data. If you lived in medieval Europe it probably mattered how many potatoes you were able to successfully harvest so you knew if you could eat most of the 120 or so days during the winter. However, it probably didn‚Äôt matter the average amount of potato rot that your village farmers experienced as a whole over the past ten years. It also probably didn‚Äôt matter how many vitamins were in your particular daily potato, since people mostly did not understand what vitamins might be. Data existed in the form of knowledge, and it made our lives better when it was communicated and shared, yet it wasn‚Äôt quantified to a great extent. During most of human history, if you broadly understood the seasons and the risks associated with catastrophic dangers, or knew someone that did, then you would be okay. You did not need detailed data that dealt in minutia. Don‚Äôt touch dead bodies. Strange dogs are bad. Fire can be used to melt metal into useful tools. That‚Äôs it, you were pretty much good to go.\nThen along came art. Art could arguably be said to have ‚Äòmetrics‚Äô before many other human endeavors. Metrics are standards of measurement, and units of measurement can comprise data. Yet even the most advanced art was taught from master to protege in a manner that had more feeling than precision. A person developed their senses by modeling off of a professional in order to produce sculpture, architecture and painting. You knew something ‚Äúworked‚Äù if your master said it did, your patron was happy, or else your creation clearly had not crumbled to the ground.\nBookkeeping and accounting could be said to hold data but its purpose was local. Don‚Äôt bankrupt the business. Buy low and sell high. The amount and types of data were necessarily limited to the types of goods and profit/loss statements. These were ‚Äúobserved‚Äù and maintained by a merchant class that probably did not care very much about making quantitative predictions about what could happen a year or years from now. Money lending is an extremely old profession, and the data from these records would have been meticulously curated. However, the data was in local and at most regional repositories.\nEntire civilizations and empires were born, prospered and withered with these kinds of attitudes to the information that they generated. It actually worked out fairly well. Many professions, not just art, had master practitioners and the heuristics of what worked and what did not could be passed down to each generation. Often this was from parent to child, that most natural extension beyond one‚Äôs own limited human lifetime of 100 years at best. Data was percolating in a biological hard drive. It lived and thrived in the human brain but was unnamed in many ways and its full potential untapped.\nThe purposeful deliberate study of phenomena with predictions and results by humans started to change the status of data on the earth. In short, the founding of the modern sciences was profound in its impact. Alchemy as an art became chemistry, which required units of measure for observations. Astrology became astronomy, folklore became history and so on. If a person is going to study an event and predict what happens, then they necessarily need to record what existed prior and what existed after. The circumstances and the results. Writing had already been around for eons. However, instead of mere bookkeeping for only in many cases one port, one city, the records of what transpired take on a greater significance. A person will conduct an experiment and share the results effectively with the world. As modern printing developed this only set data on an exponential path. Books became a preferred storage medium as paper became less costly and bookbinding became less tedious and expensive. Reproducible results could be distributed and shared. The data contained in an algorithm (otherwise known as a recipe) will get you predictable results. A science book is full of facts that support premises that support arguments. Books, books, books became vessels full of data; many, many things that we know to be true because they have been rigorously tested and the resulting data does not lie.\n\nDuring recent centuries how all of these known or assumed things can be recorded and stored has changed forever due to the fact that digital storage is effectively forever. Books fade and crumble, digits do not. Digital storage is cheap and only getting cheaper. Everything you do or say, especially on the internet, can be recorded and aggregated with millions of other beings. It is compiled, cleansed and analyzed. It is visualized with all colors imagined and those actually created with the compiled data.\nSo why exactly is data so wonderful? Today, in the modern age, it moves us forward. Data is wonderful because of the insights that it provides to engineering and other sciences. Dry statistics have more romance than the most delicate rain-kissed bouquet of flowers. This is because they are promises kept, with a healthy margin of error for good measure. Statistics have fed and clothed more people than any church.\nModern data analysis has quietly revolutionized the way we live, creating abundance not by making more, but by using what we have far more intelligently. From farms that use satellite data to grow more food with less water, to hospitals that harness patient data for faster diagnoses and personalized treatments, data is fueling smarter decisions across every industry. It helps businesses cut waste, governments deliver services more efficiently, and individuals make better choices in everything from fitness to finances. By turning raw information into insight, data analysis is unlocking innovation, driving progress, and spreading opportunity in ways that touch nearly every corner of modern life.\nData has also become, in the modern age, a fearsome monster that annihilates with precision or randomness. Statistics were used without mercy or remorse by Nazi Germany to quantify murder, mayhem and heartbreak. How many Jews could live in this town? Just take a sample of a one mile radius and you will get an answer that is ‚Äúclose enough.‚Äù This data was so meticulously recorded and stored that we still have much of it today. Other people could not possibly resist the efficiency and effectiveness of data to the point that they would risk being exposed, blamed, captured, executed. Strange measurements they used as well; were the skulls of Slavs really smaller? The records, the data, are all there even from the visits to the smallest, most obscure shtetl. Data used for evil purposes.\nIf one was to compare Amazon, inc. to the Nazis, then it would be a stretch. Fair enough. Yet the reach and impact of Amazon and other online retailers is staggering in consequences. They have contributed to abundance, and debt, in such a manner unimagined. Amazon and others can do this through data. Datasets lead to prediction models. How does Amazon know what you want to buy? Every purchase, every time something is not purchased, is saved as data to define you. Algorithms are automatically applied using millions of rows and columns. Predictions appear like magic in front of you, influencing how you spend your money and time. Influencing how many of your neighbors spend their money and time. Half the world, using online commerce or else being used by it for their data. That data is precious, that data is gold. Organizations, and your own government, don‚Äôt have to guess what you will buy, do or say. If there is data about you, or even someone a lot like you, it can be regressed, predicted or clustered.\nData is beautiful, yet awful in this visage. How can both be true? How humans recognize patterns in data is important to explaining the contradictions. It is part of our nature. Inescapable as the tide, this pattern recognition will always take place. We live, thrive and survive based on patterns and habits. When we see patterns in nature then we live to pass on our genes. What we recognize and become emotional about tends to happen. What happens when we get angry and act on that emotion? Generally bad things. Hateful things. Data and its patterns are only what humans make it. By being the best humans possible only then do we eliminate the possible bias and dangers associated with data analysis.\n\nYet now we are becoming ‚Äúmetahuman‚Äù and the consequences are yet to be seen. AI has taken data analysis and pattern recognition to the next level, turning what used to be slow, manual number-crunching into fast, intelligent insight generation. Yet not a human ‚Äúgeneration.‚Äù Entire genomes of species are discovered faster and more accurately than a human can do. AI can spot patterns in huge, messy datasets ‚Äî like images, text, or sensor data ‚Äî that humans would miss, and it doesn‚Äôt just describe what happened, it predicts what‚Äôs likely to happen next. Whether it‚Äôs catching fraud, improving customer experiences, or helping doctors make better diagnoses, AI is making data smarter, faster, and more useful than ever. It‚Äôs like giving your data a brain ‚Äî and it‚Äôs changing everything from business to healthcare to everyday life. Yet do we need to give it decision making power too? It is still too early to tell. Humans have boundless compassion and an odd hankering for malice. Computer chips have no such hang-ups.\nData has never been more valuable, and neither has our humanity. It is up to those of that insist on compiling it and using it to safeguard it. We are at an inflection point from which there is no turning back."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "I help people understand and use data models. Some of my favorite subjects to write about include data, technology and human affairs.\nOne effective structure for online content is provide an overview of the problem, then contrast it with the perfect endstate or a positive vision of the future. Finally, provide a step-by-step solution that gets your reader from the problem to the endstate.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression for Gentle Souls\n\n\n\nR\n\ncode\n\nregression\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 17, 2025\n\n\nJohn T. Miller\n\n\n\n\n\n\n\n\n\n\n\n\nThe Double-Edged Code: How Data Became Both Our Salvation and Our Undoing\n\n\n\nphilosophy\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 9, 2025\n\n\nJohn T. Miller\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Workflow Anyway?\n\n\n\ndata\n\nPython\n\nR\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 28, 2025\n\n\nJohn T. Miller\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Complexity? Well, It Is Not Complicated.\n\n\n\nphilosophy\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 15, 2025\n\n\nJohn T. Miller\n\n\n\n\n\n\n\n\n\n\n\n\nEasily Create and Change Columns in Your Data\n\n\n\nR\n\ncode\n\ndplyr\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 12, 2025\n\n\nJohn T. Miller\n\n\n\n\n\n\n\n\n\n\n\n\nA New and Improved R: Cool Use Cases of R 4.5.0 Upgrades\n\n\n\nR\n\ncode\n\nstatistics\n\n\n\n\n\n\n\n\n\nApr 19, 2025\n\n\nJohn T. Miller\n\n\n\n\n\n\n\n\n\n\n\n\nPeak Shiny Performance\n\n\n\nR\n\ncode\n\nweb apps\n\n\n\n\n\n\n\n\n\nApr 19, 2025\n\n\nJohn T. Miller\n\n\n\n\n\n\n\n\n\n\n\n\nThe Tidyverse Rocks\n\n\n\nR\n\ncode\n\ntidyverse\n\n\n\n\n\n\n\n\n\nApr 3, 2024\n\n\nJohn T. Miller\n\n\n\n\n\n\n\n\n\n\n\n\nDad Bod Be Gone\n\n\n\nfitness\n\nhealth\n\nanalysis\n\n\n\n\n\n\n\n\n\nDec 28, 2022\n\n\nJohn T. Miller\n\n\n\n\n\n\n\n\n\n\n\n\nBuild a Data Science Business\n\n\n\nR\n\nPython\n\ndata\n\n\n\n\n\n\n\n\n\nAug 24, 2022\n\n\nJohn T. Miller\n\n\n\n\n\nNo matching items"
  }
]